{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1635643,"sourceType":"datasetVersion","datasetId":966962}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\nfrom pathlib import Path\nimport random\nimport time\nimport csv\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom datasets import Dataset, Image as ImageFeature\nfrom transformers import SamModel, SamProcessor\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torchvision.transforms import InterpolationMode\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device: \", device)\n\nimport transformers, torchvision\nprint(\"Transformers Version:\", transformers.__version__)\nprint(\"Torch Version:\", torch.__version__)\nprint(\"Torchvision Version:\", torchvision.__version__)","metadata":{"_uuid":"e37e8ce9-9cae-47f7-80ae-b5281a634aa8","_cell_guid":"7a726828-5348-4551-98fd-2944decef08c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:03:42.235399Z","iopub.execute_input":"2026-01-12T09:03:42.235955Z","iopub.status.idle":"2026-01-12T09:04:10.730582Z","shell.execute_reply.started":"2026-01-12T09:03:42.235926Z","shell.execute_reply":"2026-01-12T09:04:10.729934Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration","metadata":{"_uuid":"72cb2660-bac6-4e92-96d9-0aaffe94f206","_cell_guid":"4cbc3a99-7de3-4077-9dbf-83842fc70426","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# If false, U-Net is used\nCHOOSE_SAM = True\n\n# Use weights in cross entropy to counteract class imbalance\nUSE_WEIGHTED_LOSS = True\nWEIGHTS_CALC_METHOD = \"median\"\nWEIGHT_SCALAR = 0.2\n\nUSE_FOCAL_LOSS = False  # No effect if not USE_WEIGHTED_LOSS\nFOCAL_LOSS_GAMMA = 2.0\n\n# Reproduebility\nSEED = 42\n\n# image sets\nTARGET_IMAGE_SIZE = (1024, 1024)\n\n# Train set statistics for seed 42\nMEAN = [0.4093, 0.3789, 0.2801]\nSTD = [0.1428, 0.1080, 0.0972]\n\n# dataloaders\nBATCH_SIZE = 2\nNUM_WORKERS = 2\n\n# training\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 3\nPATIENCE = 3\n","metadata":{"_uuid":"daae3232-2878-45f5-9471-b7b93316b9fe","_cell_guid":"b9763e65-f045-4526-bf5f-633682c0161a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:29.22607Z","iopub.execute_input":"2026-01-12T09:04:29.22673Z","iopub.status.idle":"2026-01-12T09:04:29.231416Z","shell.execute_reply.started":"2026-01-12T09:04:29.2267Z","shell.execute_reply":"2026-01-12T09:04:29.230701Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Log Config","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"TRAINING CONFIGURATION SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"Model:          {'SAM (head only)' if CHOOSE_SAM else 'U-Net (full)'}\")\nprint(f\"Loss:           {f'FocalLoss (γ={FOCAL_LOSS_GAMMA})' if USE_FOCAL_LOSS and USE_WEIGHTED_LOSS else 'Weighted CE' if USE_WEIGHTED_LOSS else 'CE'}\")\nprint(f\"Class weights:  {'Yes (median)' if USE_WEIGHTED_LOSS else 'No'}\")\nprint(f\"Reproducibility:{' SEED=' + str(SEED) if 'SEED' in locals() else 'No seed'}\")\nprint(f\"Image size:     {TARGET_IMAGE_SIZE}\")\nprint(f\"Normalization:  mean={MEAN}, std={STD}\")\nprint(\"-\" * 60)\nprint(f\"Batch size:     {BATCH_SIZE}\")\nprint(f\"Workers:        {NUM_WORKERS}\")\nprint(f\"Optimizer:      AdamW(lr={LEARNING_RATE}, wd=1e-4)\")\nprint(f\"Scheduler:      ReduceLROnPlateau(patience=5)\")\nprint(f\"Epochs:         {NUM_EPOCHS}\")\nprint(f\"Early stop:     Patience={PATIENCE}\")\nprint(\"=\" * 60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = Path(kagglehub.dataset_download(\"balraj98/deepglobe-land-cover-classification-dataset\"))\nprint(\"Path to the dataset: \", DATA_DIR.absolute())\n\nmetadata_df = pd.read_csv(DATA_DIR / \"metadata.csv\")\nprint(\"First rows of metadata:\")\ndisplay(metadata_df.head())","metadata":{"_uuid":"62653bc2-0cec-4367-b53f-1b80f753e131","_cell_guid":"118a1aad-d023-4b15-9eeb-286ae8501bc6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:32.135872Z","iopub.execute_input":"2026-01-12T09:04:32.136555Z","iopub.status.idle":"2026-01-12T09:04:33.025357Z","shell.execute_reply.started":"2026-01-12T09:04:32.136517Z","shell.execute_reply":"2026-01-12T09:04:33.024624Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labeled_df = metadata_df.dropna(subset=[\"sat_image_path\", \"mask_path\"]).copy()\nfor key in [\"sat_image_path\", \"mask_path\"]:\n    labeled_df[key] = labeled_df[key].apply(lambda p: str(DATA_DIR / str(p)))\n\n# Fractions\ntrain_frac = 0.70\nvalid_frac = 0.15\ntest_frac  = 0.15\n\n# First: train vs (valid+test)\ntrain_df, temp_df = train_test_split(\n    labeled_df,\n    test_size=valid_frac + test_frac,\n    random_state=SEED,\n    shuffle=True,\n)\n\n# Second: valid vs test inside temp\nvalid_df, test_df = train_test_split(\n    temp_df,\n    test_size=test_frac / (valid_frac + test_frac),\n    random_state=SEED,\n    shuffle=True,\n)\n\nsplits = {\n    \"train\": train_df,\n    \"valid\": valid_df,\n    \"test\":  test_df,\n}\n\nfor name, df in splits.items():\n    print(f\"{name.capitalize()} size:\", len(df))","metadata":{"_uuid":"90b2dd74-134e-46f1-a5ca-1a4cea6c471a","_cell_guid":"0d245fee-bb85-44d3-9c7a-125ed23bc422","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:35.496867Z","iopub.execute_input":"2026-01-12T09:04:35.497663Z","iopub.status.idle":"2026-01-12T09:04:35.529144Z","shell.execute_reply.started":"2026-01-12T09:04:35.497631Z","shell.execute_reply":"2026-01-12T09:04:35.528345Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_df = pd.read_csv(DATA_DIR / \"class_dict.csv\")\nclass_df = class_df.reset_index(names=[\"class_id\"])\n\nprint(\"Classes Table:\")\ndisplay(class_df)\nLABEL_MAP = class_df.set_index('class_id')['name'].to_dict()\n\nrgb_tuples = class_df[[\"r\", \"g\", \"b\"]].apply(tuple, axis=1)\nDEEPGLOBE_CLASSES = dict(zip(rgb_tuples, class_df[\"class_id\"]))\n\nID_TO_RGB = {class_id: rgb for rgb, class_id in DEEPGLOBE_CLASSES.items()}\nprint(\"RGB -> class_id: \", DEEPGLOBE_CLASSES)\nprint(\"class_id -> RGB: \", ID_TO_RGB)","metadata":{"_uuid":"7a929f57-98cd-4bc6-8445-ae763fd33b05","_cell_guid":"48228fbd-efc0-4673-b2fd-26a79a876071","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:37.425256Z","iopub.execute_input":"2026-01-12T09:04:37.425562Z","iopub.status.idle":"2026-01-12T09:04:37.451223Z","shell.execute_reply.started":"2026-01-12T09:04:37.425532Z","shell.execute_reply":"2026-01-12T09:04:37.45052Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_rgb_to_class_id(pil_mask: Image.Image) -> np.ndarray:\n    mask_np = np.array(pil_mask, dtype=np.uint8)\n    label_map = np.full(mask_np.shape[:2], fill_value=6, dtype=np.int64)\n\n    for rgb_tuple, class_id in DEEPGLOBE_CLASSES.items():\n        is_class = np.all(mask_np == np.array(rgb_tuple, dtype=np.uint8), axis=-1)\n        label_map[is_class] = class_id\n    return label_map\n\nprint(\"Function map_rgb_to_class_id defined.\")","metadata":{"_uuid":"4efe0f8c-4ac5-4748-8220-df5cca56d676","_cell_guid":"8716b750-e707-4369-8bc5-9ee73cf4e2e7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:39.22588Z","iopub.execute_input":"2026-01-12T09:04:39.226487Z","iopub.status.idle":"2026-01-12T09:04:39.231357Z","shell.execute_reply.started":"2026-01-12T09:04:39.226454Z","shell.execute_reply":"2026-01-12T09:04:39.23064Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_class_id_to_rgb(class_id_mask: np.ndarray) -> np.ndarray:\n    h, w = class_id_mask.shape\n    rgb = np.zeros((h,w,3), dtype=np.uint8)\n\n    for class_id, rgb_color in ID_TO_RGB.items():\n        rgb[class_id_mask == class_id] = np.array(rgb_color, dtype=np.uint8)\n    return rgb\n\nprint(\"Function decode_class_id_to_rgb defined.\")","metadata":{"_uuid":"49d8811a-dd65-4532-9879-1a681c76756c","_cell_guid":"54b34969-827c-4fed-9f0c-e14483dd8a77","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:40.147248Z","iopub.execute_input":"2026-01-12T09:04:40.147871Z","iopub.status.idle":"2026-01-12T09:04:40.152529Z","shell.execute_reply.started":"2026-01-12T09:04:40.147842Z","shell.execute_reply":"2026-01-12T09:04:40.151836Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_transform = T.Compose([\n    T.Resize(TARGET_IMAGE_SIZE, interpolation=InterpolationMode.BILINEAR),\n    T.ToTensor(),\n    T.Normalize(MEAN, STD),\n])\n\nmask_transform = T.Compose([\n    T.Resize(TARGET_IMAGE_SIZE, interpolation=InterpolationMode.NEAREST),\n])\n\nprint(\"Image and Mask transformation defined\")","metadata":{"_uuid":"d5c9e582-d18a-4eed-9b8c-6b8b9da0f746","_cell_guid":"01b4d7ec-3d3b-47e7-8c92-b2360b54c0c1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:42.301245Z","iopub.execute_input":"2026-01-12T09:04:42.301767Z","iopub.status.idle":"2026-01-12T09:04:42.306376Z","shell.execute_reply.started":"2026-01-12T09:04:42.301737Z","shell.execute_reply":"2026-01-12T09:04:42.305669Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DeepGlobeDataset(Dataset):\n    def __init__(self, dataframe, image_transform=None, mask_transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n\n        sat_path = row[\"sat_image_path\"]\n        mask_path = row[\"mask_path\"]\n\n        sat_img = Image.open(sat_path).convert(\"RGB\")\n        mask_img = Image.open(mask_path).convert(\"RGB\")\n\n        if self.image_transform is not None:\n            sat_img_tensor = self.image_transform(sat_img)\n        else:\n            sat_img_tensor = T.ToTensor()(sat_img)\n\n        if self.mask_transform is not None:\n            mask_img_transformed = self.mask_transform(mask_img)\n        else:\n            mask_img_transformed = mask_img\n\n        class_id_mask = map_rgb_to_class_id(mask_img_transformed)\n\n        labels_tensor = torch.from_numpy(class_id_mask).long()\n\n        return {\n            \"pixel_values\": sat_img_tensor,\n            \"labels\": labels_tensor,\n        }\nprint(\"DeepGlobeDataset defined.\")","metadata":{"_uuid":"e0e07f0f-79a8-410b-9e4f-1789cb7d7c85","_cell_guid":"1302b3b9-0c70-446e-8a10-509f028f2f63","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:04:44.960648Z","iopub.execute_input":"2026-01-12T09:04:44.961162Z","iopub.status.idle":"2026-01-12T09:04:44.967493Z","shell.execute_reply.started":"2026-01-12T09:04:44.96113Z","shell.execute_reply":"2026-01-12T09:04:44.96674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Datasets and Dataloaders","metadata":{"_uuid":"f7e05676-bdf4-47b2-98a6-d86d6363f932","_cell_guid":"f41ff484-269c-49d1-b6cb-712b06afde3d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"datasets = {\n    name: DeepGlobeDataset(\n        dataframe=df,\n        image_transform=image_transform,\n        mask_transform=mask_transform,\n    )\n    for name, df in splits.items()\n}\n\nfor name, ds in datasets.items():\n    print(f\"{name.capitalize()} dataset size:\", len(ds))\n\ndataloaders = {\n    \"train\": DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS),\n    \"valid\": DataLoader(datasets[\"valid\"], batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS),\n    \"test\":  DataLoader(datasets[\"test\"],  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS),\n}\n\nprint(\"Batch size:\", BATCH_SIZE)\nfor name, dl in dataloaders.items():\n    print(f\"{name.capitalize()} DataLoader created.\")","metadata":{"_uuid":"650aa4fc-664a-4430-bdff-f95aa7530a16","_cell_guid":"3b6211ac-0a81-4b18-a345-d4e3e54ae0f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:04:48.465418Z","iopub.execute_input":"2026-01-12T09:04:48.465717Z","iopub.status.idle":"2026-01-12T09:04:48.473442Z","shell.execute_reply.started":"2026-01-12T09:04:48.465691Z","shell.execute_reply":"2026-01-12T09:04:48.472723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_class_weights_from_loader(train_loader, num_classes, device=\"cpu\",\n                                      method=\"inverse\", eps=1e-6):\n\n    # Accumulate pixel counts per class\n    class_counts = torch.zeros(num_classes, dtype=torch.float64)\n\n    for batch in train_loader:\n        labels = batch[\"labels\"]            # (N, H, W)\n        # Flatten and count\n        labels_flat = labels.view(-1)\n        bincount = torch.bincount(labels_flat, minlength=num_classes).to(torch.float64)\n        class_counts += bincount\n\n    # Avoid division by zero\n    class_counts = class_counts + eps\n\n    if method == \"inverse\":\n        # w_c = 1 / f_c, then normalize to keep mean weight ~ 1\n        weights = 1.0 / class_counts\n        weights = weights / weights.mean()\n    elif method == \"median\":\n        # Median frequency balancing: w_c = median(f) / f_c  [SegNet]\n        freq = class_counts / class_counts.sum()\n        median_freq = freq.median()\n        weights = median_freq / freq\n    else:\n        raise ValueError(f\"Unknown method {method}\")\n\n    return weights.to(device=device, dtype=torch.float32)","metadata":{"_uuid":"101df772-0853-4823-b567-79768e8618c7","_cell_guid":"2836a5c6-a984-45fc-b16e-e78511c38d2b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-12T09:04:50.68581Z","iopub.execute_input":"2026-01-12T09:04:50.686443Z","iopub.status.idle":"2026-01-12T09:04:50.69234Z","shell.execute_reply.started":"2026-01-12T09:04:50.686412Z","shell.execute_reply":"2026-01-12T09:04:50.6916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.patches as mpatches\n\ndef legend_patches_for_present_classes(gt_mask_np, pred_mask_np, label_map, id_to_rgb):\n\n    present_ids = sorted(set(np.unique(gt_mask_np)).union(set(np.unique(pred_mask_np))))\n\n    patches = []\n    for cid in present_ids:\n        cid = int(cid)\n        if cid not in id_to_rgb:\n            continue\n        rgb = id_to_rgb[cid]  # (r,g,b) in 0..255\n        color01 = (rgb[0] / 255.0, rgb[1] / 255.0, rgb[2] / 255.0)\n        name = label_map.get(cid, f\"class_{cid}\")\n        patches.append(mpatches.Patch(color=color01, label=name))\n\n    return patches\n\n\ndef add_landuse_legend(ax, patches, title=\"Land-Cover Classes\", loc=\"upper left\"):\n    if not patches:\n        return\n    ax.legend(\n        handles=patches,\n        title=title,\n        loc=loc,\n        frameon=True,\n        fontsize=8,\n        title_fontsize=9,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:04:56.280484Z","iopub.execute_input":"2026-01-12T09:04:56.280999Z","iopub.status.idle":"2026-01-12T09:04:56.286814Z","shell.execute_reply.started":"2026-01-12T09:04:56.280971Z","shell.execute_reply":"2026-01-12T09:04:56.286228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define UNet","metadata":{"_uuid":"3f31effd-2ef0-4f49-9d9b-94ec4348aa73","_cell_guid":"3b658045-edae-4f78-b725-3c5b6887406f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super().__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        self.down4 = Down(512, 1024)\n\n        self.up1 = Up(1024, 512)\n        self.up2 = Up(512, 256)\n        self.up3 = Up(256, 128)\n        self.up4 = Up(128, 64)\n        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits","metadata":{"_uuid":"2b8b0ac8-50b7-48a1-91ba-795182eea0f0","_cell_guid":"afc464a5-5cad-4905-9cd6-9cc0adb65a9a","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:04:57.368829Z","iopub.execute_input":"2026-01-12T09:04:57.369145Z","iopub.status.idle":"2026-01-12T09:04:57.380489Z","shell.execute_reply.started":"2026-01-12T09:04:57.369117Z","shell.execute_reply":"2026-01-12T09:04:57.37989Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define SAM-Adaption","metadata":{"_uuid":"08aec321-77bb-4987-a809-cd89549f382b","_cell_guid":"74523147-fc62-47e6-b012-80f16bdc374b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class SegmentationHead(nn.Module):\n    def __init__(self, in_channels: int, num_classes: int, target_size):\n\n        super().__init__()\n        self.target_size = target_size\n\n        self.conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(256)\n        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.conv_out = nn.Conv2d(128, num_classes, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = F.interpolate(\n            x,\n            size=self.target_size,\n            mode=\"bilinear\",\n            align_corners=False\n        )\n\n        x = self.conv_out(x)\n        return x\nprint(\"SegmentationHead defined.\")","metadata":{"_uuid":"8bc10ca1-569e-4944-9ce0-f23390a2d3a1","_cell_guid":"cd4f847c-552f-4d2a-8d15-7ffa847da4df","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:05:03.63065Z","iopub.execute_input":"2026-01-12T09:05:03.631369Z","iopub.status.idle":"2026-01-12T09:05:03.637767Z","shell.execute_reply.started":"2026-01-12T09:05:03.631338Z","shell.execute_reply":"2026-01-12T09:05:03.636996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SamSegmentationModel(nn.Module):\n    def __init__(self, sam_image_encoder, num_classes: int, target_size):\n        super().__init__()\n        self.sam_image_encoder = sam_image_encoder\n        \n        with torch.no_grad():            \n            dummy = torch.zeros(\n                1, 3, TARGET_IMAGE_SIZE[0], TARGET_IMAGE_SIZE[1],\n                device=device,\n                dtype=next(sam_image_encoder.parameters()).dtype\n            )\n            encoder_output = self.sam_image_encoder(dummy)\n\n            if hasattr(encoder_output, \"last_hidden_state\") and encoder_output.last_hidden_state is not None:\n                feat = encoder_output.last_hidden_state   # (1, C, H_feat, W_feat)\n            elif hasattr(encoder_output, \"image_embeddings\") and encoder_output.image_embeddings is not None:\n                feat = encoder_output.image_embeddings    # (1, C, H_feat, W_feat)\n            else:\n                raise ValueError(f\"Unexpected Encoder-Output: {encoder_output}\")\n\n            if feat.dim() != 4:\n                raise ValueError(f\"Expected Feature-Shape (B, C, H, W), was: {feat.shape}\")\n\n            in_channels = feat.shape[1]\n            print(\"Channels from SAM-Encoder:\", in_channels)\n                \n        self.head = SegmentationHead(\n            in_channels=in_channels,\n            num_classes=num_classes,\n            target_size=target_size,\n        )\n\n    def forward(self, pixel_values):\n        with torch.no_grad():\n            encoder_output = self.sam_image_encoder(pixel_values)\n        \n        if hasattr(encoder_output, \"last_hidden_state\") and encoder_output.last_hidden_state is not None:\n            features = encoder_output.last_hidden_state  # (B, C, H_feat, W_feat)\n        elif hasattr(encoder_output, \"image_embeddings\") and encoder_output.image_embeddings is not None:\n            features = encoder_output.image_embeddings   # (B, C, H_feat, W_feat)\n        else:\n            raise ValueError(f\"Unexpected Encoder-Output: {encoder_output}\")\n\n        if features.dim() != 4:\n            raise ValueError(f\"Expected Feature-Shape (B, C, H, W), was: {features.shape}\")\n        \n        logits = self.head(features)  # (B, num_classes, H_out, W_out)\n\n        return logits\nprint(\"SamSegmentationModel defined.\")","metadata":{"_uuid":"87e3c383-1131-4331-9f01-ff53074dede3","_cell_guid":"eea51a1c-fdd2-4c63-af93-eae91d384330","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:05:10.541703Z","iopub.execute_input":"2026-01-12T09:05:10.54231Z","iopub.status.idle":"2026-01-12T09:05:10.550918Z","shell.execute_reply.started":"2026-01-12T09:05:10.54228Z","shell.execute_reply":"2026-01-12T09:05:10.550386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Instantiate Model","metadata":{"_uuid":"f5107478-54d0-403e-906b-13cb6678c608","_cell_guid":"7d8f45c7-761c-4adc-8154-6295ef1f6840","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"num_classes=len(DEEPGLOBE_CLASSES)\nCHECKPOINT_DIR = Path(\"./checkpoints\")\nCHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n\nif CHOOSE_SAM:\n    from transformers import SamModel, SamProcessor\n    \n    target_size = TARGET_IMAGE_SIZE\n    CHECKPOINT_PATH = Path(\"./checkpoints/sam_deepglobe_head.pt\")\n        \n    sam_model = SamModel.from_pretrained(\n        \"facebook/sam-vit-base\",\n    )\n    \n    sam_image_encoder = sam_model.vision_encoder.to(device)\n    \n    for p in sam_image_encoder.parameters():\n        p.requires_grad = False\n    sam_image_encoder.eval()\n    \n    model = SamSegmentationModel(\n        sam_image_encoder=sam_image_encoder,\n        num_classes=num_classes,\n        target_size=TARGET_IMAGE_SIZE\n    ).to(device)\n    \n    model.eval()\n    \n    # Debug\n    print(\"Hidden Channels aus SAM:\", model.head.conv1.in_channels)\n    print(\"Number of classes: \", num_classes)\nelse:\n    CHECKPOINT_PATH = Path(\"./checkpoints/UNet.pt\")\n    model = UNet(n_channels=3, n_classes=len(DEEPGLOBE_CLASSES))\n    model.to(device)","metadata":{"_uuid":"482ad763-5cdb-41f9-bb99-342df6ef49cc","_cell_guid":"1f6f8cc3-95bf-4257-9c79-8dfa9ed8d881","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:05:14.887338Z","iopub.execute_input":"2026-01-12T09:05:14.888064Z","iopub.status.idle":"2026-01-12T09:05:23.818079Z","shell.execute_reply.started":"2026-01-12T09:05:14.888017Z","shell.execute_reply":"2026-01-12T09:05:23.817319Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, weight=None, ignore_index=-100):\n        super().__init__()\n        self.gamma = gamma\n        self.weight = weight\n        self.ignore_index = ignore_index\n\n    def forward(self, logits, targets):\n        ce_loss = F.cross_entropy(logits, targets, ignore_index=self.ignore_index, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1 - pt) ** self.gamma * ce_loss\n        if self.weight is not None:\n            focal_loss = self.weight[targets] * focal_loss \n        return focal_loss.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if USE_WEIGHTED_LOSS:\n    class_weights = compute_class_weights_from_loader(\n        dataloaders['train'],\n        num_classes=len(DEEPGLOBE_CLASSES),\n        device=device,\n        method=WEIGHTS_CALC_METHOD\n    )\n    print(f\"Computed weights for Cross-Entropy: {class_weights}\")\n    if USE_FOCAL_LOSS:\n        criterion = FocalLoss(\n            gamma=FOCAL_LOSS_GAMMA,\n            weight=class_weights\n        )\n    else:\n        criterion = torch.nn.CrossEntropyLoss(weight=class_weights * WEIGHT_SCALAR)\nelse:\n    criterion = torch.nn.CrossEntropyLoss()\n    \noptimizer = torch.optim.AdamW(\n    model.parameters(), \n    lr=LEARNING_RATE,\n    weight_decay=1e-4,  \n    betas=(0.9, 0.999)  # Default Adam\n)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, \n    mode='max', \n    factor=0.5, \n    patience=5, \n    min_lr=1e-7, \n    threshold=0.001  # min improvement\n)\n\nprint(\"UNet Model, CrossEntropyLoss, and AdamW optimizer initialized.\")","metadata":{"_uuid":"e2ed7f7e-a7c7-41e3-a671-c8500cc64c62","_cell_guid":"0d7b0f43-0d51-4cb0-8c47-c84277ee2fb0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:05:25.722535Z","iopub.execute_input":"2026-01-12T09:05:25.722836Z","iopub.status.idle":"2026-01-12T09:05:25.728843Z","shell.execute_reply.started":"2026-01-12T09:05:25.72281Z","shell.execute_reply":"2026-01-12T09:05:25.728185Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fast_confusion_matrix(preds, labels, num_classes):\n    preds = preds.reshape(-1)\n    labels = labels.reshape(-1)\n        \n    mask = (labels >= 0) & (labels < num_classes)\n    labels = labels[mask]\n    preds = preds[mask]\n    \n    cm = np.bincount(\n        num_classes * labels + preds,\n        minlength=num_classes**2\n    ).reshape(num_classes, num_classes)\n\n    return cm","metadata":{"_uuid":"44642c6e-bfbe-4e68-8b75-ac0faae567eb","_cell_guid":"7a45aa6f-aaa9-4b3e-bea2-ccb988756a01","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:05:26.67581Z","iopub.execute_input":"2026-01-12T09:05:26.676638Z","iopub.status.idle":"2026-01-12T09:05:26.681098Z","shell.execute_reply.started":"2026-01-12T09:05:26.6766Z","shell.execute_reply":"2026-01-12T09:05:26.680388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_batch_metrics(logits, labels, num_classes):\n    preds = torch.argmax(logits, dim=1)\n\n    preds_np = preds.cpu().numpy()\n    labels_np = labels.cpu().numpy()\n\n    correct = (preds_np == labels_np).sum()\n    total = np.prod(labels_np.shape)\n    pixel_acc = correct / max(1, total)\n\n    ious = []\n    for c in range(num_classes):\n        pred_c = (preds_np == c)\n        label_c = (labels_np == c)\n\n        intersection = np.logical_and(pred_c, label_c).sum()\n        union = np.logical_or(pred_c, label_c).sum()\n\n        if union > 0:\n            iou_c = intersection / union\n            ious.append(iou_c)\n\n    if len(ious) > 0:\n        mean_iou = float(np.mean(ious))\n    else:\n        mean_iou = 0.0\n\n    return pixel_acc, mean_iou\nprint(\"Metrics Function Defined.\")","metadata":{"_uuid":"b4d14c0a-6aae-4ed9-abbb-69fa65876409","_cell_guid":"002b9263-f8bd-4ccc-9410-d95d2356c248","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:05:27.614043Z","iopub.execute_input":"2026-01-12T09:05:27.614837Z","iopub.status.idle":"2026-01-12T09:05:27.62075Z","shell.execute_reply.started":"2026-01-12T09:05:27.614809Z","shell.execute_reply":"2026-01-12T09:05:27.619952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    num_batches = 0\n\n    for batch_idx, batch in enumerate(dataloader):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n\n        logits = model(pixel_values)\n\n        loss = criterion(logits, labels)\n\n        loss.backward()\n\n        optimizer.step()\n\n        running_loss += loss.item()\n        num_batches += 1\n\n        if (batch_idx + 1) % 20 == 0:\n            print(f\"Batch {batch_idx + 1}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n    epoch_loss = running_loss / max(1, num_batches)\n    return epoch_loss\nprint(\"train_one_epoch function defined.\")","metadata":{"_uuid":"a51120b7-7e70-4e0c-b8a9-2134d1cc797b","_cell_guid":"7e2e4e9a-78d7-4aca-b178-e18c55b39f4e","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:05:29.831534Z","iopub.execute_input":"2026-01-12T09:05:29.831844Z","iopub.status.idle":"2026-01-12T09:05:29.837895Z","shell.execute_reply.started":"2026-01-12T09:05:29.831816Z","shell.execute_reply":"2026-01-12T09:05:29.837214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_epoch(model, dataloader, criterion, device, num_classes):\n    model.eval()\n\n    total_loss = 0.0\n    num_batches = 0\n    \n    total_cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n\n    with torch.no_grad():\n        for batch in dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            logits = model(pixel_values)\n\n            loss = criterion(logits, labels)\n\n            preds = torch.argmax(logits, dim=1)\n            \n            preds_np = preds.cpu().numpy()\n            labels_np = labels.cpu().numpy()\n\n            cm = fast_confusion_matrix(preds_np, labels_np, num_classes)\n            total_cm += cm\n\n            total_loss += loss.item()\n            num_batches += 1\n\n    avg_loss = total_loss / max(1, num_batches)\n    \n    per_class_iou = {}\n    ious = []\n\n    for k in range(num_classes):\n        tp = total_cm[k, k]\n        fp = total_cm[:, k].sum() - tp\n        fn = total_cm[k, :].sum() - tp\n        denom = tp + fp + fn\n        iou = (tp / denom) if denom > 0 else np.nan\n        per_class_iou[k] = iou\n        if not np.isnan(iou):\n            ious.append(iou)\n\n    mean_iou = float(np.mean(ious)) if len(ious) > 0 else 0.0\n    \n    correct = np.trace(total_cm)\n    total = total_cm.sum()\n    pixel_acc = (correct / total) if total > 0 else 0.0\n\n    return avg_loss, pixel_acc, mean_iou, per_class_iou, total_cm","metadata":{"_uuid":"03686b74-ba0a-40b5-b7fd-7e6093ff3334","_cell_guid":"64633374-2ba1-432a-b27f-a32f61c56830","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:05:30.899142Z","iopub.execute_input":"2026-01-12T09:05:30.899726Z","iopub.status.idle":"2026-01-12T09:05:30.907086Z","shell.execute_reply.started":"2026-01-12T09:05:30.899696Z","shell.execute_reply":"2026-01-12T09:05:30.906347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\nimport time\n\ndef save_final_test_report(run_dir, run_name, choose_sam, test_loss, test_acc, test_miou,\n                           per_class_iou, test_cm, label_map, extra_config=None):\n    run_dir = Path(run_dir)\n    out_dir = run_dir / \"final_report\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    k = int(test_cm.shape[0])\n    class_names = [label_map.get(i, f\"class_{i}\") for i in range(k)]\n    support_pixels = test_cm.sum(axis=1).astype(np.int64)\n\n    # per-class IoU CSV\n    per_df = pd.DataFrame({\n        \"class_id\": np.arange(k, dtype=int),\n        \"class_name\": class_names,\n        \"support_pixels\": support_pixels,\n        \"iou\": [float(per_class_iou.get(i, np.nan)) for i in range(k)],\n    })\n    per_df.to_csv(out_dir / \"per_class_iou.csv\", index=False)\n\n    # confusion matrix raw\n    np.save(out_dir / \"confusion_matrix.npy\", test_cm)\n\n    # summary JSON\n    summary = {\n        \"run_name\": run_name,\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"choose_sam\": bool(choose_sam),\n        \"test_loss\": float(test_loss),\n        \"pixel_accuracy\": float(test_acc),\n        \"mIoU\": float(test_miou),\n        \"num_classes\": k,\n        \"config\": extra_config or {},\n    }\n    with open(out_dir / \"summary.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(summary, f, indent=2, ensure_ascii=False)\n\n    # one-line file for quick comparisons\n    comp_path = run_dir / \"final_report\" / \"comparison_row.csv\"\n    pd.DataFrame([{\n        \"run_name\": run_name,\n        \"choose_sam\": summary[\"choose_sam\"],\n        \"test_loss\": summary[\"test_loss\"],\n        \"pixel_accuracy\": summary[\"pixel_accuracy\"],\n        \"mIoU\": summary[\"mIoU\"],\n    }]).to_csv(comp_path, index=False)\n\n    print(f\"[OK] Final report saved: {out_dir}\")\n    return out_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:05:34.334715Z","iopub.execute_input":"2026-01-12T09:05:34.335247Z","iopub.status.idle":"2026-01-12T09:05:34.343665Z","shell.execute_reply.started":"2026-01-12T09:05:34.335217Z","shell.execute_reply":"2026-01-12T09:05:34.343116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop","metadata":{"_uuid":"cb654545-50aa-4bf7-9695-5a00cfb5191d","_cell_guid":"04c18b11-b2fb-4aa9-9d2c-b01cb19b6829","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"train_loss_history = []\nval_loss_history = []\nval_pixel_acc_history = []\nval_miou_history = []\nbest_val_miou = -1.0\npatience_counter = 0\n\nprint(\"Logging lists initialized.\")\n\nrun_dir = Path(\"runs\") / time.strftime(\"%Y-%m-%d_%H-%M-%S\")\nrun_dir.mkdir(parents=True, exist_ok=True)\n\n# TensorBoard\ntb_writer = SummaryWriter(log_dir=str(run_dir))\n\nper_class_cols = [f\"iou_class_{k}\" for k in range(num_classes)]\n\n# CSV\ncsv_path = run_dir / \"metrics.csv\"\ncsv_fields = [\n    \"epoch\",\n    \"train_loss\",\n    \"val_loss\",\n    \"val_pixel_acc\",\n    \"val_miou\",\n    \"epoch_time_sec\",\n    \"lr\",\n    \"weight_decay\",\n    \"loss_gap\",\n    \"miou_num_valid_classes\",    \n] + per_class_cols\n\nwith csv_path.open(\"w\", newline=\"\") as f:\n    w = csv.DictWriter(f, fieldnames=csv_fields)\n    w.writeheader()\n\n# ============================================\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n==============================\")\n    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n    print(f\"==============================\")\n\n    epoch_start = time.time()\n\n    # training\n    train_loss = train_epoch(\n        model=model,\n        dataloader=dataloaders['train'],\n        optimizer=optimizer,\n        criterion=criterion,\n        device=device\n    )\n\n    # validation\n    val_loss, val_acc, val_miou, per_class_iou, val_cm = eval_epoch(\n        model=model,\n        dataloader=dataloaders['valid'],\n        criterion=criterion,\n        device=device,\n        num_classes=num_classes\n    )\n\n    scheduler.step(val_miou)\n\n    epoch_time = time.time() - epoch_start\n    lr = optimizer.param_groups[0][\"lr\"]\n    wd = optimizer.param_groups[0].get(\"weight_decay\", 0.0)\n\n    valid_iou_vals = [v for v in per_class_iou.values() if v == v]\n    miou_num_valid_classes = len(valid_iou_vals)\n\n    # --- TensorBoard Scalars ---\n    tb_writer.add_scalar(\"loss/train\", train_loss, epoch + 1)\n    tb_writer.add_scalar(\"loss/val\", val_loss, epoch + 1)\n    tb_writer.add_scalar(\"metrics/val_pixel_acc\", val_acc, epoch + 1)\n    tb_writer.add_scalar(\"metrics/val_miou\", val_miou, epoch + 1)\n    tb_writer.add_scalar(\"time/epoch_sec\", epoch_time, epoch + 1)\n    tb_writer.add_scalar(\"opt/lr\", lr, epoch + 1)\n    tb_writer.flush()\n\n    # --- CSV row ---\n    row = {\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"val_pixel_acc\": val_acc,\n        \"val_miou\": val_miou,\n        \"epoch_time_sec\": epoch_time,\n        \"lr\": lr,\n        \"weight_decay\": wd,\n        \"loss_gap\": train_loss - val_loss,\n        \"miou_num_valid_classes\": miou_num_valid_classes,\n    }\n\n    for k in range(num_classes):\n        v = per_class_iou.get(k, float(\"nan\"))\n        row[f\"iou_class_{k}\"] = (float(v) if v == v else \"\")\n    \n    with csv_path.open(\"a\", newline=\"\") as f:\n        w = csv.DictWriter(f, fieldnames=csv_fields)\n        w.writerow(row)\n\n    # Prints\n    print(f\"Train loss: {train_loss:.4f}\")\n    print(\"Validation:\")\n    print(f\"  Loss: {val_loss:.4f}\")\n    print(f\"  PixelAcc: {val_acc:.4f}\")\n    print(f\"  mIoU: {val_miou:.4f}\")\n\n    train_loss_history.append(train_loss)\n    val_loss_history.append(val_loss)\n    val_pixel_acc_history.append(val_acc)\n    val_miou_history.append(val_miou)\n\n    # Checkpointing\n    if val_miou > best_val_miou:\n        best_val_miou = val_miou\n\n        if CHOOSE_SAM:\n            torch.save({\n                \"epoch\": epoch,\n                \"head_state_dict\": model.head.state_dict(),   # <─ only the head\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"loss\": val_loss,\n                \"accuracy\": val_acc,\n                \"iou\": val_miou,\n            }, CHECKPOINT_PATH)\n        else:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': val_loss,\n                'accuracy': val_acc,\n                'iou': val_miou,\n            }, CHECKPOINT_PATH)\n        print(f\"Model saved! Current best validation IoU: {best_val_miou:.4f}\")\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        print(f\"Patience: No improvements since {patience_counter} epochs.\")\n        \n    if patience_counter > PATIENCE:\n        print(f\"Patience limit reached. Terminating Training...\")\n        break\n\ntb_writer.close()\nprint(\"Logs saved in:\", run_dir)","metadata":{"_uuid":"b3f47b12-5e41-48e0-9ccc-8ae11bc6ed55","_cell_guid":"0c2f8df5-510a-4f96-86a4-10257d94ecc9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-12T09:05:41.227093Z","iopub.execute_input":"2026-01-12T09:05:41.227676Z","iopub.status.idle":"2026-01-12T09:14:57.487408Z","shell.execute_reply.started":"2026-01-12T09:05:41.227649Z","shell.execute_reply":"2026-01-12T09:14:57.48654Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load(CHECKPOINT_PATH, weights_only=False)\n\nif CHOOSE_SAM:\n    model.head.load_state_dict(checkpoint[\"head_state_dict\"])\nelse:\n    model.load_state_dict(checkpoint['model_state_dict'])\n\nprint(\"Best model loaded successfully.\")","metadata":{"_uuid":"d3bd248e-bbd8-45f7-a95d-2ed7ff29a577","_cell_guid":"394bc810-68e0-4c83-ad01-709c87c99053","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:15:02.295934Z","iopub.execute_input":"2026-01-12T09:15:02.296522Z","iopub.status.idle":"2026-01-12T09:15:02.315699Z","shell.execute_reply.started":"2026-01-12T09:15:02.296483Z","shell.execute_reply":"2026-01-12T09:15:02.315109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{"_uuid":"3ba7e4b3-ccb0-43b8-90d8-00dcd0bfb726","_cell_guid":"675011db-1ee9-4b4d-aaba-733548d4c67a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"test_loss, test_acc, test_miou, per_class_iou, test_cm = eval_epoch(\n    model=model,\n    dataloader=dataloaders['test'],\n    criterion=criterion,\n    device=device,\n    num_classes=num_classes\n)\n\nrun_name = (\"SAM\" if CHOOSE_SAM else \"UNET\") + f\"_seed{SEED}_bs{BATCH_SIZE}_ep{NUM_EPOCHS}\"\n\nextra_config = {\n    \"seed\": SEED,\n    \"batch_size\": BATCH_SIZE,\n    \"epochs\": NUM_EPOCHS,\n    \"target_image_size\": TARGET_IMAGE_SIZE,\n    \"weighted_loss\": USE_WEIGHTED_LOSS,\n    \"lr\": optimizer.param_groups[0][\"lr\"],\n}\n\nfinal_dir = save_final_test_report(\n    run_dir=run_dir,\n    run_name=run_name,\n    choose_sam=CHOOSE_SAM,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_miou=test_miou,\n    per_class_iou=per_class_iou,\n    test_cm=test_cm,\n    label_map=LABEL_MAP,\n    extra_config=extra_config,\n)\n\nprint(f\"\\n--- Test Metrics (Best Model) ---\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test IoU: {test_miou:.4f}\")\nprint(f\"Per Class IoU: {per_class_iou}\")","metadata":{"_uuid":"579db840-97a9-4dea-adce-63b8aee70317","_cell_guid":"d0bdbb6e-7ea5-4d02-adcf-efe0b8f1538a","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:15:04.713458Z","iopub.execute_input":"2026-01-12T09:15:04.714138Z","iopub.status.idle":"2026-01-12T09:15:37.750572Z","shell.execute_reply.started":"2026-01-12T09:15:04.71411Z","shell.execute_reply":"2026-01-12T09:15:37.749684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_random_examples(dataloader, model, device, num_examples=6):\n    model.eval()\n    collected = 0\n\n    fig, axes = plt.subplots(\n        num_examples, 3, figsize=(15, 4 * num_examples),\n        gridspec_kw={\"width_ratios\": [1, 1, 1.2]}\n    )\n\n    if num_examples == 1:\n        axes = np.expand_dims(axes, axis=0)\n\n    with torch.no_grad():\n        for batch in dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            logits = model(pixel_values)\n            preds = torch.argmax(logits, dim=1)\n\n            for i in range(pixel_values.size(0)):\n                if collected >= num_examples:\n                    break\n                \n                img = pixel_values[i].cpu().permute(1, 2, 0).numpy()\n                img = (img * np.array(STD) + np.array(MEAN))\n                img = np.clip(img, 0.0, 1.0)\n\n                gt_mask_np = labels[i].cpu().numpy()\n                pred_mask_np = preds[i].cpu().numpy()\n\n                gt_rgb = decode_class_id_to_rgb(gt_mask_np)\n                pred_rgb = decode_class_id_to_rgb(pred_mask_np)\n\n                patches = legend_patches_for_present_classes(\n                    gt_mask_np,\n                    pred_mask_np,\n                    LABEL_MAP,\n                    ID_TO_RGB\n                )\n\n                ax_img, ax_gt, ax_pred = axes[collected]\n                \n                ax_img.imshow(img)\n                ax_img.set_title(\"Image\")\n                ax_img.axis(\"off\")\n\n                ax_gt.imshow(gt_rgb)\n                ax_gt.set_title(\"Ground Truth\")\n                ax_gt.axis(\"off\")\n\n                ax_pred.imshow(pred_rgb)\n                ax_pred.set_title(\"Prediction\")\n                ax_pred.axis(\"off\")\n                \n                ax_pred.legend(\n                    handles=patches,\n                    title=\"Land Cover\",\n                    loc=\"center left\",\n                    bbox_to_anchor=(1.02, 0.5),\n                    frameon=True,\n                    fontsize=8,\n                    title_fontsize=9\n                )\n\n                collected += 1\n\n            if collected >= num_examples:\n                break\n\n    plt.tight_layout()\n    plt.show()\nshow_random_examples(dataloaders['test'], model, device, num_examples=6)","metadata":{"_uuid":"c6972e94-5815-4423-b95f-8878682452a2","_cell_guid":"4439fe72-a45f-4865-8a6b-efa2208c2c83","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T09:17:03.04041Z","iopub.execute_input":"2026-01-12T09:17:03.040791Z","iopub.status.idle":"2026-01-12T09:17:09.691375Z","shell.execute_reply.started":"2026-01-12T09:17:03.040757Z","shell.execute_reply":"2026-01-12T09:17:09.690153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(\n    cm,\n    class_names=None,\n    normalize=None,\n    title=\"Confusion Matrix\",\n    label_map=None,\n):\n\n    cm = cm.astype(np.float64)\n\n    if normalize == \"true\":\n        denom = cm.sum(axis=1, keepdims=True)\n        cm = np.divide(cm, denom, out=np.zeros_like(cm), where=denom != 0)\n        fmt = \".2f\"\n        plot_title = title + \" (row-normalized)\"\n    elif normalize == \"pred\":\n        denom = cm.sum(axis=0, keepdims=True)\n        cm = np.divide(cm, denom, out=np.zeros_like(cm), where=denom != 0)\n        fmt = \".2f\"\n        plot_title = title + \" (col-normalized)\"\n    elif normalize == \"all\":\n        denom = cm.sum()\n        cm = cm / denom if denom != 0 else cm\n        fmt = \".3f\"\n        plot_title = title + \" (global-normalized)\"\n    else:\n        fmt = \"d\"\n        plot_title = title\n\n    plt.figure(figsize=(7, 6))\n    plt.imshow(cm, interpolation=\"nearest\")\n    plt.title(plot_title)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.colorbar()\n\n    k = cm.shape[0]\n\n    if class_names is None:\n        class_names = list(range(k))\n\n    if label_map is not None:\n        class_names = [label_map.get(c, str(c)) for c in class_names]\n\n    plt.xticks(np.arange(k), class_names, rotation=45, ha=\"right\")\n    plt.yticks(np.arange(k), class_names)\n\n    thresh = (cm.max() * 0.6) if cm.size > 0 else 0\n    for i in range(k):\n        for j in range(k):\n            val = cm[i, j]\n            txt = format(int(val), fmt) if fmt == \"d\" else format(val, fmt)\n            plt.text(\n                j,\n                i,\n                txt,\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if val > thresh else \"black\",\n            )\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"55c4ddfc-ca2b-4a28-aa29-0a194f68697a","_cell_guid":"f349c594-08b4-4286-99b7-486e15313d97","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:55:09.355354Z","iopub.execute_input":"2026-01-10T10:55:09.355697Z","iopub.status.idle":"2026-01-10T10:55:09.36566Z","shell.execute_reply.started":"2026-01-10T10:55:09.355665Z","shell.execute_reply":"2026-01-10T10:55:09.364974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# raw counts\nplot_confusion_matrix(test_cm, title=\"Test Confusion Matrix\", label_map=LABEL_MAP)\n\nplot_confusion_matrix(test_cm, normalize=\"pred\", title=\"Test Confusion Matrix\", label_map=LABEL_MAP)\n\nplot_confusion_matrix(test_cm, normalize=\"true\", title=\"Test Confusion Matrix\", label_map=LABEL_MAP)\n\nplot_confusion_matrix(test_cm, normalize=\"all\", title=\"Test Confusion Matrix\", label_map=LABEL_MAP)","metadata":{"_uuid":"059d06e5-fb10-4033-95a6-25bce5e2d087","_cell_guid":"b0748fd0-2aa2-4013-8d38-25007b8bb139","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-10T10:55:10.199272Z","iopub.execute_input":"2026-01-10T10:55:10.199813Z","iopub.status.idle":"2026-01-10T10:55:11.154792Z","shell.execute_reply.started":"2026-01-10T10:55:10.19978Z","shell.execute_reply":"2026-01-10T10:55:11.154223Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(csv_path)\n\nepochs = df[\"epoch\"].values\ntrain_loss_history = df[\"train_loss\"].values\nval_loss_history = df[\"val_loss\"].values\nval_pixel_acc_history = df[\"val_pixel_acc\"].values\nval_miou_history = df[\"val_miou\"].values\n\n# 1) Loss\nplt.figure(figsize=(7, 4))\nplt.plot(epochs, train_loss_history, label=\"Train Loss\")\nplt.plot(epochs, val_loss_history, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over epochs\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# 2) Pixel Accuracy\nplt.figure(figsize=(7, 4))\nplt.plot(epochs, val_pixel_acc_history, label=\"Val Pixel Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Pixel Accuracy\")\nplt.title(\"Validation Pixel Accuracy over epochs\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# 3) Mean IoU\nplt.figure(figsize=(7, 4))\nplt.plot(epochs, val_miou_history, label=\"Val Mean IoU\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Mean IoU\")\nplt.title(\"Validation Mean IoU over epochs\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"_uuid":"a9ebb270-0633-44dc-83b1-12cc12dc85a2","_cell_guid":"038eb21c-4cc5-4bc6-a179-6de5fc886622","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-10T10:55:15.140969Z","iopub.execute_input":"2026-01-10T10:55:15.141572Z","iopub.status.idle":"2026-01-10T10:55:15.531155Z","shell.execute_reply.started":"2026-01-10T10:55:15.141542Z","shell.execute_reply":"2026-01-10T10:55:15.530616Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}